Implementation of a neural network and of backpropagation.  
The goal is to produce and train a network with 3 layers:  input - hidden - output.  Both the input layer and the output layer will have 8 nodes, the hidden layer only 3 nodes (+biases).The learning examples will each have 7 zeros and 1 one in them (so there will be only 8 different learning examples,and you will have to repeat them) and the output the network should learn is exactly the same as the input.  So whenthe input layer is given<0,0,0,1,0,0,0,0>as input, the output to aim for is also<0,0,0,1,0,0,0,0>.
Trying to get your network to learn this reproducing function on the 8 different learning examples.Then study the weights and the activations of the hidden nodes of network and try to interpret them. 

First we initialise the matrixes for the three layers with random weights between 0 and 1 and all biases as 0. Then we implement the forward propagation method that sends the sum of the weights times the input values through the sigmoid function. The backpropagation method iterates backwards through the network layers and calculates the delta cumulatively and saves it for each weight and finally the gradient descent method updates the weights.

To train the network, we first initialise the 8 , 3 and 8 neurons for each of the 3 layers respectively. For a batch of 8 samples carrying all the possible different value combination, for each of input we forward propagate to the hidden and output layers, then calculate the error as the difference between the target output and the sample output. We backpropagate it through the layers, and apply the gradient descent method with a parameter tuning the learning rate. This process happens for a number of epochs specified in the initalisation. 

Finally we keep track of the cumulative error rate per epoch for plotting reasons.
The number of epochs as well as the learning rates are hard coded in the main method of the code and two hard coded examples of predictions are exemplified after the training has finished.
