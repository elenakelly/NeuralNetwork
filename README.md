First we initialise the matrixes for the three layers with random weights between 0 and 1 and all biases as 0. Then we implement the forward propagation method that sends the sum of the weights times the input values through the sigmoid function. The backpropagation method iterates backwards through the network layers and calculates the delta cumulatively and saves it for each weight and finally the gradient descent method updates the weights.
To train the network, we first initialise the 8 , 3 and 8 neurons for each of the 3 layers respectively. For a batch of 8 samples carrying all the possible different value combination, for each of input we forward propagate to the hidden and output layers, then calculate the error as the difference between the target output and the sample output. We backpropagate it through the layers, and apply the gradient descent method with a parameter tuning the learning rate. This process happens for a number of epochs specified in the initalisation. Finally we keep track of the cumulative error rate per epoch for plotting reasons.
The number of epochs as well as the learning rates are hard coded in the main method of the code and two hard coded examples of predictions are exemplified after the training has finished.